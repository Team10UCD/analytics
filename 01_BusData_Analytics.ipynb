{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics - Bus Data Understanding and Prep\n",
    "\n",
    "## Audit\n",
    "\n",
    "Author: Danning\n",
    "\n",
    "Last Modified By: Adam\n",
    "\n",
    "Module: COMP47630\n",
    "\n",
    "DC:     2021-06-20\n",
    "\n",
    "DLM:    2021-06-23\n",
    "\n",
    "Desc:   This file contains an analysis of the historic bus data\n",
    "\n",
    "Dict:   The Data Dictionary for the Data Set is available in Brightspace\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "00. Introduction\n",
    "\n",
    "01. Exec Summary and Results\n",
    "\n",
    "02. Modules\n",
    "\n",
    "03. Constants\n",
    "\n",
    "04. Ingestion\n",
    "\n",
    "05. Cleansing\n",
    "\n",
    "\n",
    "## 00. Introduction\n",
    "\n",
    "### 00.01 Background\n",
    "(here)\n",
    "\n",
    "### 00.02 Problem Scope\n",
    "(here)\n",
    "\n",
    "### 00.03 Data\n",
    "(here)\n",
    "\n",
    "### 00.04 Approach\n",
    "(here)\n",
    "\n",
    "## 01. Exec Summary\n",
    "\n",
    "(here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ------BEGIN---------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.01 Modules\n",
    "Import all modules here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/team10/miniconda3/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####--------------------------------------\n",
    "#00.Import Modules\n",
    "####--------------------------------------\n",
    "\n",
    "######---------BEGIN\n",
    "#      SUPPRESS DEPRECIATION WARNINGS: Applicable to datetime_is_numeric=True\n",
    "######--------END\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "######---------BEGIN\n",
    "#      ML\n",
    "######--------END\n",
    "\n",
    "#import nltk as nl\n",
    "import sklearn as sk\n",
    "import matplotlib as mp\n",
    "#import xgboost as xg\n",
    "#import pymc3 as pymc\n",
    "#import sympy as sym\n",
    "\n",
    "\n",
    "\n",
    "######---------BEGIN\n",
    "#      SQL/API\n",
    "######--------END\n",
    "\n",
    "\n",
    "#import requests as rq\n",
    "#import sqlalchemy as sqla\n",
    "#import pyodbc\n",
    "#import cx_oracle as cx\n",
    "\n",
    "\n",
    "######---------BEGIN\n",
    "#     GENERAL\n",
    "######--------END\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from dask import dataframe as dask_df\n",
    "#import pyspark as spk\n",
    "#import json\n",
    "#import time\n",
    "#import socket\n",
    "#import traceback as tb\n",
    "#import platform\n",
    "#from psutil import virtual_memory\n",
    "import pickle as pck\n",
    "\n",
    "\n",
    "######---------BEGIN\n",
    "#     VISUALISATIONS\n",
    "######--------END\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "#For showing plots directly in the notebook run the command below\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "###HTML Output Hiding\n",
    "#Install hide input extension\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user\n",
    "#!jupyter nbextension enable hide_input_all/main\n",
    "#!jupyter nbextension enable hide_input/main\n",
    "#!jupyter nbextension enable codefolding/main\n",
    "\n",
    "#!jupyter nbextension disable hide_input_all/main\n",
    "#!jupyter nbextension disable hide_input/main\n",
    "#!jupyter nbextension disable codefolding/main\n",
    "\n",
    "#Update with Filename: Run in Terminal, post completion, after hiding all cells for report\n",
    "#!jupyter nbconvert --to=html bus_Data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.02 Constants\n",
    "Import all Constants here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_year='2018'\n",
    "bus_leavetimes_filepath=\"./data/rt_leavetimes_DB_{}.txt\".format(data_year)\n",
    "bus_trips_filepath=\"./data/rt_trips_DB_{}.txt\".format(data_year)\n",
    "\n",
    "bus_leavetimes_sep=\";\"\n",
    "bus_trips_sep=\";\"\n",
    "\n",
    "bus_leavetimes_data_dictionary={\n",
    "        'DATASOURCE':['Description','category']\n",
    "        ,'DAYOFSERVICE':['Description','datetime']\n",
    "        ,'TRIPID':['Description','category']\n",
    "        ,'PROGRNUMBER':['Description','category']\n",
    "        ,'STOPPOINTID':['Description','category']\n",
    "        ,'PLANNEDTIME_ARR':['Description','int64']\n",
    "        ,'PLANNEDTIME_DEP':['Description','int64']\n",
    "        ,'ACTUALTIME_ARR':['Description','int64']\n",
    "        ,'ACTUALTIME_DEP':['Description','int64']\n",
    "        ,'VEHICLEID':['Description','category']\n",
    "        ,'PASSENGERS':['Description','float64']\n",
    "        ,'PASSENGERSIN':['Description','float64']\n",
    "        ,'PASSENGERSOUT':['Description','float64']\n",
    "        ,'DISTANCE': ['Description','float64']\n",
    "        ,'SUPPRESSED':['Description','category']\n",
    "        ,'JUSTIFICATIONID':['Description','category']\n",
    "        ,'LASTUPDATE':['Description','datetime']\n",
    "        ,'NOTE': ['Description','string']\n",
    "                        }\n",
    "\n",
    "\n",
    "lt_datetime_columns=[column_headers for column_headers, column_desc_array in bus_leavetimes_data_dictionary.items() if column_desc_array[1] == 'datetime']\n",
    "lt_categorical_columns=[column_headers for column_headers, column_desc_array in bus_leavetimes_data_dictionary.items() if column_desc_array[1] == 'category']\n",
    "lt_num_columns=[column_headers for column_headers, column_desc_array in bus_leavetimes_data_dictionary.items() if column_desc_array[1] in ('numeric','int64','float64')]\n",
    "\n",
    "\n",
    "\n",
    "bus_leavetimes_metadata_dictionary={}\n",
    "\n",
    "for column_headers, column_desc_array in bus_leavetimes_data_dictionary.items():\n",
    "    bus_leavetimes_metadata_dictionary[column_headers]=column_desc_array[1]\n",
    "    \n",
    "    \n",
    "bus_trips_data_dictionary={\n",
    "        'DATASOURCE':['Description','category']\n",
    "        ,'DAYOFSERVICE':['Description','datetime']\n",
    "        ,'TRIPID':['Description','category']\n",
    "        ,'LINEID':['Description','category']\n",
    "        ,'ROUTEID':['Description','category']\n",
    "    \n",
    "        ,'DIRECTION':['Description','category']\n",
    "        \n",
    "        ,'PLANNEDTIME_ARR':['Description','int64']\n",
    "        ,'PLANNEDTIME_DEP':['Description','int64']\n",
    "        ,'ACTUALTIME_ARR':['Description','int64']\n",
    "        ,'ACTUALTIME_DEP':['Description','int64']\n",
    "    \n",
    "        ,'BASIN':['Description','category']\n",
    "        ,'TENDERLOT':['Description','float64']\n",
    "    \n",
    "        ,'SUPPRESSED':['Description','category']\n",
    "        ,'JUSTIFICATIONID':['Description','category']\n",
    "        ,'LASTUPDATE':['Description','datetime']\n",
    "        ,'NOTE': ['Description','string']\n",
    "                        }\n",
    "\n",
    "bus_trips_metadata_dictionary={}\n",
    "\n",
    "for column_headers, column_desc_array in bus_trips_data_dictionary.items():\n",
    "    bus_trips_metadata_dictionary[column_headers]=column_desc_array[1]\n",
    "\n",
    "    \n",
    "tp_datetime_columns=[column_headers for column_headers, column_desc_array in bus_trips_data_dictionary.items() if column_desc_array[1] == 'datetime']\n",
    "tp_categorical_columns=[column_headers for column_headers, column_desc_array in bus_trips_data_dictionary.items() if column_desc_array[1] == 'category']\n",
    "tp_num_columns=[column_headers for column_headers, column_desc_array in bus_trips_data_dictionary.items() if column_desc_array[1] in ('numeric','int64','float64')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#Dates for File Additions if needed\n",
    "today_date=dt.datetime.now()\n",
    "\n",
    "#DateTime objects\n",
    "today_year=today_date.year\n",
    "today_month=today_date.month\n",
    "today_day=today_date.day\n",
    "\n",
    "#Convert to ISO Standard for Filename\n",
    "str_year=str(today_date.year)\n",
    "\n",
    "#Month should have two digits\n",
    "str_month=str(today_date.month)\n",
    "if len(str_month)==1:\n",
    "    str_month=\"0{}\".format(str_month)\n",
    "\n",
    "#Day should have two digits\n",
    "str_day=str(today_date.day)\n",
    "if len(str_day)==1:\n",
    "    str_day=\"0{}\".format(str_day)\n",
    "\n",
    "\n",
    "str_today_date=\"{}-{}-{}\".format(str_year,str_month,str_day)\n",
    "\n",
    "datetime_format='%d-%b-%Y %H:%M:%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsk_describe(dask_df_iter):\n",
    "    \"\"\"A function to describe dask df\"\"\"\n",
    "    \n",
    "    \n",
    "    desc_df=dask_df_iter.describe(include='all')  \n",
    "    desc_df=desc_df.compute().T\n",
    "    display(type(desc_df))\n",
    "    \n",
    "    return desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def missing_check(row):\n",
    "    \"\"\"Highlight rows with potential missing_values\"\"\"\n",
    "\n",
    "    #Configuration Values\n",
    "    col_to_check=len(row)-1\n",
    "    default_colour = 'white'\n",
    "    flag_colour=''\n",
    "    high_flag_colour_val='red'\n",
    "    med_flag_colour_val='orange'\n",
    "    low_flag_colour_val='yellow'\n",
    "    val_to_check=0\n",
    "\n",
    "    #Row length valid\n",
    "    if len(row)>=col_to_check:\n",
    "\n",
    "        #\n",
    "        if row.values[col_to_check] == 'High':\n",
    "            flag_colour = high_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Medium':\n",
    "            flag_colour = med_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Low':\n",
    "            flag_colour = low_flag_colour_val\n",
    "\n",
    "        if flag_colour=='':\n",
    "            colour=default_colour\n",
    "        else:\n",
    "            colour=flag_colour\n",
    "\n",
    "        return ['background-color: {}'.format(colour)]*len(row.values)\n",
    "\n",
    "    else:\n",
    "        print('Row too short - Reconfigure Column Number')\n",
    "        return ['background-color: {}'.format(default_colour)]*len(row.values)\n",
    "    \n",
    "def dt_missing_check(row):\n",
    "    \"\"\"Highlight rows with potential missing_values\"\"\"\n",
    "\n",
    "    #Configuration Values\n",
    "    col_to_check=len(row)-1\n",
    "    default_colour = 'white'\n",
    "    flag_colour=''\n",
    "    high_flag_colour_val='red'\n",
    "    med_flag_colour_val='orange'\n",
    "    low_flag_colour_val='yellow'\n",
    "    val_to_check=0\n",
    "\n",
    "    #Row length valid\n",
    "    if len(row)>=col_to_check:\n",
    "\n",
    "        #\n",
    "        if row.values[col_to_check] == 'High':\n",
    "            flag_colour = high_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Medium':\n",
    "            flag_colour = med_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Low':\n",
    "            flag_colour = low_flag_colour_val\n",
    "\n",
    "        if flag_colour=='':\n",
    "            colour=default_colour\n",
    "        else:\n",
    "            colour=flag_colour\n",
    "\n",
    "        return ['background-color: {}'.format(colour)]*len(row.values)\n",
    "\n",
    "    else:\n",
    "        print('Row too short - Reconfigure Column Number')\n",
    "        return ['background-color: {}'.format(default_colour)]*len(row.values)\n",
    "    \n",
    "\n",
    "def ingest_data(fp,delim,data_dictionary,chunks=10000000,pandas=False):\n",
    "    \"\"\"A function to read in CSV Data and Validate.\n",
    "    \n",
    "    Memory error after 50M rows\"\"\"\n",
    "\n",
    "    print(\"Inside ingest_data({},dictionary)\".format(fp))\n",
    "    \n",
    "    def print_shape(raw_df):\n",
    "        \"\"\"A function to print the shape of a dataframe\"\"\"\n",
    "        #row_column data\n",
    "        shape_of_df=raw_df.shape\n",
    "        row_count=shape_of_df[0]\n",
    "        column_count=shape_of_df[1]\n",
    "\n",
    "        #print info to user\n",
    "        row_column_print_statement='Your file contains: \\n{} rows x {} columns.\\n\\n'\n",
    "        row_column_print_statement=row_column_print_statement.format(row_count,column_count)\n",
    "        print(row_column_print_statement)\n",
    "        header_statement='The following columns are present:\\n'\n",
    "\n",
    "        #print the headers\n",
    "        for header in raw_df.columns:\n",
    "            header_statement+='\"{}\"\\n'.format(header)\n",
    "\n",
    "        print(header_statement)\n",
    "        return\n",
    "        \n",
    "    def verify_schema(raw_df,data_dictionary):\n",
    "        \"\"\"A function to validate the schema of a dataframe\"\"\"\n",
    "        \n",
    "        match=False\n",
    "        #check if the schema is correct\n",
    "        if set(raw_df.columns)==set(data_dictionary.keys()) and len(raw_df.columns)==len(data_dictionary.keys()):\n",
    "            print('The columns in this data sample match the schema')\n",
    "            match=True\n",
    "\n",
    "        else:\n",
    "            print('The columns in this data sample do not match the schema')\n",
    "            \n",
    "        return match\n",
    "    \n",
    "    def unique_values(df):\n",
    "        \"\"\"A function to print the unique values in each column\"\"\"\n",
    "        \n",
    "        print_statement=\"\"\"\\n\\n-----\\n\\nColumn: {}\\n\\nValues: {}\\n\\n\"\"\"\n",
    "        \n",
    "        for column in df.columns:\n",
    "            print_statement.format(column,df[column].unique())\n",
    "            \n",
    "        return\n",
    "\n",
    "    def descriptive_stats(df,num_datetime=False,pandas=True):\n",
    "        \"\"\"A function to get descriptive stats for a dataframe\"\"\"\n",
    "        \n",
    "        #Format Dictionary:\n",
    "        non_numeric_format_dictionary={\n",
    "                              '% Populated': \"{:.0f}%\"\n",
    "                              ,'% Missing': \"{:.0f}%\"\n",
    "                                ,'% Top Value':\"{:.0f}%\"}\n",
    "        \n",
    "        numeric_format_dictionary={'count':\"{:.0f}\"\n",
    "                 ,'% Populated': \"{:.0f}%\"\n",
    "                ,'% Missing': \"{:.0f}%\"\n",
    "                ,'% Top Value':\"{:.0f}%\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        row_count=df.shape[0]\n",
    "        \n",
    "        #Produce a summary table - Note the continuous features in this dataset are dates\n",
    "        #Note: As of now, pandas allows using describe with dates so keeping it in, but this will be depreciated\n",
    "        if pandas:\n",
    "            category_summary_df=df.describe(datetime_is_numeric=num_datetime).T\n",
    "            \n",
    "        else:\n",
    "            category_summary_df=dsk_describe(dask_df_iter=df)\n",
    "            \n",
    "        #Add what Percent is populated\n",
    "        category_summary_df['% Populated']=100*(category_summary_df['count']/row_count)\n",
    "\n",
    "        #Percent missing\n",
    "        category_summary_df['% Missing']=100-category_summary_df['% Populated']\n",
    "\n",
    "        #Prevelance of top vlaue\n",
    "        category_summary_df['% Top Value']=100*(category_summary_df['freq']/row_count)\n",
    "\n",
    "        #Give a warning depending on quartile of missing data - upper quartiles are high\n",
    "        category_summary_df['Missing Warning']=np.select([(category_summary_df['% Missing']==0),(category_summary_df['% Missing']>0) & (category_summary_df['% Missing']<25),(category_summary_df['% Missing']>=25) & (category_summary_df['% Missing']<50),category_summary_df['% Missing']>=50],['None','Low','Medium','High'])\n",
    "\n",
    "        category_summary_df=category_summary_df.reset_index()\n",
    "        category_summary_df=category_summary_df.rename(columns={\"index\": \"feature\"})\n",
    "\n",
    "        if num_datetime:\n",
    "            print('Datetime: Numeric')\n",
    "            display((category_summary_df.style.apply(missing_check, axis=1)\n",
    "                                                 .format(non_numeric_format_dictionary)))\n",
    "        \n",
    "        else:\n",
    "            print('Datetime: NotNumeric')\n",
    "            display((category_summary_df.style.apply(dt_missing_check, axis=1)\n",
    "                                         .format(numeric_format_dictionary)))\n",
    "        \n",
    "        \n",
    "        return category_summary_df\n",
    "        \n",
    "\n",
    "    if pandas==True:\n",
    "        #Valid Filepath\n",
    "        if os.path.isfile(fp):\n",
    "\n",
    "            raw_df = pd.DataFrame()\n",
    "            chunk_count=0\n",
    "            \n",
    "            for chunk in pd.read_csv(fp,sep=delim,dtype=str,chunksize=chunks):\n",
    "                chunk_count+=1\n",
    "                print('On Chunk: {}'.format(chunk_count))\n",
    "                raw_df = pd.concat([raw_df,chunk])\n",
    "                \n",
    "            display(raw_df)\n",
    "\n",
    "            print_shape(raw_df)\n",
    "\n",
    "            verify_schema(raw_df,data_dictionary)\n",
    "            \n",
    "            unique_values(raw_df)\n",
    "            \n",
    "            descriptive_stats(raw_df,num_datetime=False)\n",
    "\n",
    "            print(\"\\n\\n\\nSample Data:\\n\\n\\n\")\n",
    "            display(raw_df.head())\n",
    "            \n",
    "            return raw_df\n",
    "\n",
    "        #Not Valid Filepath\n",
    "        else:\n",
    "            print(\"Invalid filepath - Correct the filepath and re-ingest\")\n",
    "\n",
    "            return\n",
    "        \n",
    "    elif pandas==False:\n",
    "        \n",
    "        #Valid Filepath\n",
    "        if os.path.isfile(fp):\n",
    "\n",
    "            #read_csv - Do Not Let Pandas Manipulate the Data First - Auto-assign is more memory intensive.\n",
    "            raw_df=dask_df.read_csv(fp, sep=delim)\n",
    "            display(raw_df)\n",
    "\n",
    "            print_shape(raw_df)\n",
    "\n",
    "            verify_schema(raw_df,data_dictionary)\n",
    "            \n",
    "            unique_values(raw_df)\n",
    "\n",
    "            descriptive_stats(raw_df,num_datetime=False,pandas=False)\n",
    "            descriptive_stats(raw_df,num_datetime=True,pandas=False)\n",
    "            \n",
    "            print(\"\\n\\n\\nSample Data:\\n\\n\\n\")\n",
    "            display(raw_df.head())\n",
    "\n",
    "            return raw_df\n",
    "\n",
    "        #Not Valid Filepath\n",
    "        else:\n",
    "            print(\"Invalid filepath - Correct the filepath and re-ingest\")\n",
    "\n",
    "            return\n",
    "        \n",
    "    else:\n",
    "        print('No opinion on using Dask or Pandas - Defaulting to Dask')\n",
    "        \n",
    "        #Valid Filepath\n",
    "        if os.path.isfile(fp):\n",
    "\n",
    "            #read_csv - Do Not Let Pandas Manipulate the Data First - Auto-assign is more memory intensive.\n",
    "            raw_df=dask_df.read_csv(fp, sep=delim)\n",
    "            display(raw_df)\n",
    "\n",
    "            print_shape(raw_df)\n",
    "\n",
    "            verify_schema(raw_df,data_dictionary)\n",
    "            \n",
    "            unique_values(raw_df)\n",
    "            \n",
    "            descriptive_stats(raw_df,num_datetime=False)\n",
    "            descriptive_stats(raw_df,num_datetime=True)\n",
    "            \n",
    "            \n",
    "\n",
    "            print(\"\\n\\n\\nSample Data:\\n\\n\\n\")\n",
    "            display(raw_df.head())\n",
    "            \n",
    "            return raw_df\n",
    "        \n",
    "        #Not Valid Filepath\n",
    "        else:\n",
    "            print(\"Invalid filepath - Correct the filepath and re-ingest\")\n",
    "\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_convert(df,types,columnlist,dt_format):\n",
    "    \"\"\"A function to convert all columns in a list into the appropriate type\"\"\"\n",
    "    \n",
    "    print(\"Inside data_convert()\")\n",
    "    \n",
    "    ###Check if empty\n",
    "    if len(df.index) != 0:\n",
    "        \n",
    "        ##Check if datetime or other\n",
    "        if types=='datetime':\n",
    "       \n",
    "            ###Check if 0\n",
    "            if len(columnlist)>0:\n",
    "                print('Converting to {}'.format(types))\n",
    "                \n",
    "                for column in columnlist:\n",
    "                    df[column]=df[column].apply(pd.to_datetime,format=dt_format,errors='ignore')\n",
    "                \n",
    "            else:\n",
    "                print('No need to convert to: {}'.format(types))\n",
    "              \n",
    "        ###Numeric type\n",
    "        elif types=='category':\n",
    "            ###Check if 0\n",
    "            if len(columnlist)>0:\n",
    "                print('Converting to {}'.format(types))\n",
    "                \n",
    "                for column in columnlist:\n",
    "                    df[column]=df[column].astype('category')\n",
    "                \n",
    "            ###Nothing to convert\n",
    "            else:\n",
    "                print('No need to convert')\n",
    "                \n",
    "        ###Numeric type\n",
    "        elif types=='numeric':\n",
    "            \n",
    "            ###Check if 0\n",
    "            if len(columnlist)>0:\n",
    "                print('Converting to Numerical')\n",
    "                \n",
    "                for column in columnlist:\n",
    "                    df[column]=df[column].apply(pd.to_numeric, errors='ignore')\n",
    "                \n",
    "            else:\n",
    "                print('No need to convert')\n",
    "                \n",
    "        ###Other type - e.g. Boolean, string - Dont do anything - force the above types.\n",
    "        else:\n",
    "            print('Unknown type')\n",
    "                \n",
    "    ###Empty data set          \n",
    "    else:\n",
    "        print(\"Empty dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_over_single_categories(df,categorical_columns,pdf_fn, save_fig=True):\n",
    "    \"\"\"A function to group over the categories\"\"\"\n",
    "    \n",
    "    print(\"Inside group_over_single_categories()\")\n",
    "    row_count=len(df)\n",
    "\n",
    "    grouping_type={}\n",
    "    timestamp_now=dt.datetime.timestamp(dt.datetime.now())\n",
    "    \n",
    "    #Dataframe is not empty, and there are categorical columns to group over:\n",
    "    if df.empty==False and len(categorical_columns)>0:\n",
    "        with PdfPages(pdf_fn) as pp: #lab\n",
    "            column=''\n",
    "\n",
    "            #Let's go through the category column type\n",
    "            for column in categorical_columns:\n",
    "\n",
    "                #Separator\n",
    "                print('\\n\\n----------------------\\n\\n')\n",
    "                agg_df=df.groupby([column]).agg({df.columns[0]:\"count\"})\n",
    "                print(agg_df)\n",
    "                agg_df=agg_df.rename(columns={df.columns[0]:'Rows'})\n",
    "                agg_df=agg_df.reset_index()\n",
    "\n",
    "                #Note: Could also do value_counts but I prefer that for graphing.\n",
    "                agg_df['% Frequency']=100*(agg_df['Rows']/row_count)\n",
    "\n",
    "                #Be explicit over what we're displaying\n",
    "                print('Grouping over {} results in:\\n'.format(column))\n",
    "\n",
    "                #Display the result\n",
    "                display(agg_df)\n",
    "                \n",
    "                print('Non Zero Data:')\n",
    "                non_zero_df=agg_df[agg_df['Rows']>0]\n",
    "                display(non_zero_df)\n",
    "                #Graphing Section:\n",
    "\n",
    "                if len(agg_df)<300:\n",
    "                    figure = (\n",
    "                                df[column]\n",
    "                                  .value_counts(dropna=True, normalize=True)\n",
    "                                  .plot(kind='bar'\n",
    "                                        ,title='Count of values for {}'.format(column)\n",
    "                                        , xlabel='Field Values'\n",
    "                                        , ylabel='Count of Values'\n",
    "                                        , figsize=(35,35)\n",
    "                                       )\n",
    "                     )\n",
    "                    #This grid style is from the sample Lab5 as I like how it looks\n",
    "                    plt.ylim([0,1])\n",
    "                    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "                    plt.setp(figure.get_xticklabels(), ha=\"right\", rotation=0)\n",
    "                    plt.minorticks_on()\n",
    "                    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "                    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    "                    plt.show()\n",
    "                    grouping_type[column]=agg_df\n",
    "\n",
    "                    if save_fig:\n",
    "                        pp.savefig(figure.get_figure())\n",
    "            else:\n",
    "                print('Too Many Categories to plot')\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return grouping_type\n",
    "\n",
    "def group_over_multi_categories(df,categorical_columns,pdf_fn,save_output=False,save_fig=False):\n",
    "    \"\"\"A function to group over all pairs of categories\n",
    "    \n",
    "    Warning: This can be memory intensive as we have (columnCount)C(2) pairings, so only run this if your device is able!\"\"\"\n",
    "    \n",
    "    print(\"Inside group_over_multi_categories()\")\n",
    "    row_count=len(df)\n",
    "    grouping_type={}\n",
    "    timestamp_now=dt.datetime.timestamp(dt.datetime.now())\n",
    "    \n",
    "    #Try run this\n",
    "    try:\n",
    "    \n",
    "        #Dataframe is not empty, and there are categorical columns to group over:\n",
    "        if df.empty==False and len(categorical_columns)>0:\n",
    "            with PdfPages(pdf_fn) as pp:\n",
    "                column=''\n",
    "                second_column=''\n",
    "                \n",
    "                #Let's go through the category column type\n",
    "                for column in categorical_columns:\n",
    "\n",
    "                    #Second index, n^2\n",
    "                    for second_column in categorical_columns:\n",
    "                        multi_column=[column]\n",
    "\n",
    "\n",
    "                        #Create a key to access - pipe delimited as columns contain _\n",
    "                        grouping_key=\"{}|{}\"\n",
    "\n",
    "                        #No point in grouping the same column twice\n",
    "                        if second_column!=column and  column not in ('TRIPID','DATASOURCE') and second_column not in ('TRIPID','DATASOURCE'):\n",
    "                            multi_column+=[second_column]\n",
    "                            grouping_key=grouping_key.format(column,second_column)\n",
    "\n",
    "                            #Separator\n",
    "                            print('\\n\\n----------------------\\n\\n')\n",
    "                            agg_df=df.groupby(multi_column).agg({df.columns[0]:\"count\"})\n",
    "                            agg_df=agg_df.rename(columns={df.columns[0]:'Rows'})\n",
    "                            agg_df['% Frequency']=100*(agg_df['Rows']/row_count)\n",
    "                            agg_df=agg_df.reset_index()\n",
    "\n",
    "                            #Be explicit over what we're displaying\n",
    "                            print('Grouping over {} results in:\\n'.format(grouping_key))\n",
    "\n",
    "                            #Display the result\n",
    "                            display(agg_df)\n",
    "                            \n",
    "                            print('Non Zero Data:')\n",
    "                            non_zero_df=agg_df[agg_df['Rows']>0]\n",
    "                            display(non_zero_df)\n",
    "                            \n",
    "                            #Graph\n",
    "                            \n",
    "                            if len(agg_df)<50000:\n",
    "                                figure = (\n",
    "                                            (df[multi_column]\n",
    "                                                  .dropna()\n",
    "                                                  .value_counts(normalize=True)\n",
    "                                                  .reset_index()\n",
    "                                                  .pivot_table(index=column,columns=second_column)\n",
    "                                                  .fillna(0))[0]\n",
    "                                                          .plot(kind='bar'\n",
    "                                                            , stacked=True\n",
    "                                                            , title='Count of values for {} vs {}'.format(second_column,column)\n",
    "                                                            , xlabel='Field Values'\n",
    "                                                            , ylabel='Count of Values'\n",
    "                                                            , figsize=(35,35)\n",
    "\n",
    "\n",
    "                                                               )\n",
    "                                         )\n",
    "\n",
    "                                #This grid style is from the sample Lab5 as I like how it looks\n",
    "                                plt.ylim([0,1])\n",
    "                                plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "                                plt.setp(figure.get_xticklabels(), ha=\"right\", rotation=0)\n",
    "                                plt.minorticks_on()\n",
    "                                plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "                                plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    "                                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                                if save_fig:\n",
    "                                    pp.savefig(figure.get_figure())\n",
    "\n",
    "                                #Only save if explicitly passed - This could kill your memory.\n",
    "                            if save_output:\n",
    "                                grouping_type[grouping_key]=agg_df\n",
    "\n",
    "                        \n",
    "    #Catch exceptions\n",
    "    except Exception as exc:\n",
    "        print(\"Function exception:\\n\")\n",
    "        #check exception is memory error\n",
    "        if exc==MemoryError:\n",
    "            print(\"Sorry, your device is not able to run this function as you have hit a memory limit\")\n",
    "            \n",
    "        print(exc)\n",
    "        \n",
    "\n",
    "    return grouping_type\n",
    "\n",
    "\n",
    "def cat_missing_check_cleanse(row):\n",
    "    \"\"\"Highlight rows with potential missing_values\"\"\"\n",
    "\n",
    "    #Configuration Values\n",
    "    col_to_check=8\n",
    "    default_colour = 'green'\n",
    "    flag_colour=''\n",
    "    high_flag_colour_val='red'\n",
    "    med_flag_colour_val='orange'\n",
    "    low_flag_colour_val='yellow'\n",
    "    val_to_check=0\n",
    "\n",
    "    #Row length valid\n",
    "    if len(row)>=col_to_check:\n",
    "\n",
    "        #\n",
    "        if row.values[col_to_check] == 'High':\n",
    "            flag_colour = high_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Medium':\n",
    "            flag_colour = med_flag_colour_val\n",
    "            \n",
    "        elif row.values[col_to_check] == 'Low':\n",
    "            flag_colour = low_flag_colour_val\n",
    "\n",
    "        if flag_colour=='':\n",
    "            colour=default_colour\n",
    "        else:\n",
    "            colour=flag_colour\n",
    "\n",
    "        return ['background-color: {}'.format(colour)]*len(row.values)\n",
    "\n",
    "    else:\n",
    "        print('Row too short - Reconfigure Column Number')\n",
    "        return ['background-color: {}'.format(default_colour)]*len(row.values)\n",
    "    \n",
    "def stacked_group_over_multi_categories(df,categorical_columns,pdf_fn,save_output=False,save_fig=False):\n",
    "    \"\"\"A function to group over all pairs of categories\n",
    "    \n",
    "    Warning: This can be memory intensive as we have (columnCount)C(2) pairings, so only run this if your device is able!\"\"\"\n",
    "    \n",
    "    print(\"Inside group_over_multi_categories()\")\n",
    "    row_count=len(df)\n",
    "    grouping_type={}\n",
    "    timestamp_now=dt.datetime.timestamp(dt.datetime.now())\n",
    "    \n",
    "    #Try run this\n",
    "    try:\n",
    "    \n",
    "        #Dataframe is not empty, and there are categorical columns to group over:\n",
    "        if df.empty==False and len(categorical_columns)>0:\n",
    "            with PdfPages(pdf_fn) as pp:\n",
    "                column=''\n",
    "                second_column=''\n",
    "                \n",
    "                #Let's go through the category column type\n",
    "                for column in categorical_columns:\n",
    "\n",
    "                    #Second index, n^2\n",
    "                    for second_column in categorical_columns:\n",
    "                        multi_column=[column]\n",
    "\n",
    "\n",
    "                        #Create a key to access - pipe delimited as columns contain _\n",
    "                        grouping_key=\"{}|{}\"\n",
    "\n",
    "                        #No point in grouping the same column twice\n",
    "                        if second_column!=column:\n",
    "                            multi_column+=[second_column]\n",
    "                            grouping_key=grouping_key.format(column,second_column)\n",
    "\n",
    "                            #Separator\n",
    "                            print('\\n\\n----------------------\\n\\n')\n",
    "                            agg_df=df.groupby(multi_column).agg({df.columns[0]:\"count\"})\n",
    "                            agg_df=agg_df.rename(columns={df.columns[0]:'Rows'})\n",
    "                            agg_df['% Frequency']=100*(agg_df['Rows']/row_count)\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            #Be explicit over what we're displaying\n",
    "                            print('Grouping over {} results in:\\n'.format(grouping_key))\n",
    "\n",
    "                            #Display the result\n",
    "                            display(agg_df)\n",
    "                            \n",
    "                            print('Non Zero Data:')\n",
    "                            non_zero_df=agg_df[agg_df['Rows']>0]\n",
    "                            display(non_zero_df)\n",
    "                            \n",
    "                            agg_df=agg_df.reset_index()\n",
    "\n",
    "                            sagg_df=(\n",
    "                                    df\n",
    "                                    .groupby([column])\n",
    "                                    .agg({df.columns[0]:\"count\"})\n",
    "                                    .reset_index()\n",
    "                                    .rename(columns={df.columns[0]:'TotalRows'})\n",
    "                                    )\n",
    "\n",
    "                            join_df=agg_df.merge(sagg_df,left_on=column,right_on=column,suffixes=('_subbed','_group'))\n",
    "                            join_df['% Stacked']=join_df['Rows']/join_df['TotalRows']\n",
    "\n",
    "                            figure=((join_df\n",
    "                                  .pivot_table(index=column,columns=second_column,values='% Stacked')\n",
    "                                  .fillna(0))\n",
    "\n",
    "                                          .plot(kind='bar'\n",
    "                                            , stacked=True\n",
    "                                            , title='Distribution of values for {} vs {}'.format(second_column,column)\n",
    "                                            , xlabel='Field Values'\n",
    "                                            , ylabel='Makeup of Values'\n",
    "                                            , figsize=(35,35)\n",
    "\n",
    "                            ))\n",
    "\n",
    "                            #This grid style is from the sample Lab5 as I like how it looks\n",
    "                            plt.ylim([0,1])\n",
    "                            plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "                            plt.setp(figure.get_xticklabels(), ha=\"right\", rotation=0)\n",
    "                            plt.minorticks_on()\n",
    "                            plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "                            plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    "                            plt.show()\n",
    "\n",
    "                            \n",
    "\n",
    "                            if save_fig:\n",
    "                                pp.savefig(figure.get_figure())\n",
    "\n",
    "                            #Only save if explicitly passed - This could kill your memory.\n",
    "                            if save_output:\n",
    "                                grouping_type[grouping_key]=agg_df\n",
    "\n",
    "                        \n",
    "    #Catch exceptions\n",
    "    except Exception as exc:\n",
    "        print(\"Function exception:\\n\")\n",
    "        #check exception is memory error\n",
    "        if exc==MemoryError:\n",
    "            print(\"Sorry, your device is not able to run this function as you have hit a memory limit\")\n",
    "            \n",
    "        print(exc)\n",
    "        \n",
    "\n",
    "    return grouping_type\n",
    "\n",
    "\n",
    "def stacked_group_over_target_categories(df,categorical_columns,pdf_fn,save_output=False,save_fig=False):\n",
    "    \"\"\"A function to group over all pairs of categories and the target death_yn\n",
    "    \n",
    "    Warning: This can be memory intensive as we have so only run this if your device is able!\"\"\"\n",
    "    \n",
    "    print(\"Inside group_over_multi_categories()\")\n",
    "    row_count=len(df)\n",
    "    grouping_type={}\n",
    "    timestamp_now=dt.datetime.timestamp(dt.datetime.now())\n",
    "    \n",
    "    #Try run this\n",
    "    try:\n",
    "    \n",
    "        #Dataframe is not empty, and there are categorical columns to group over:\n",
    "        if df.empty==False and len(categorical_columns)>0:\n",
    "            with PdfPages(pdf_fn) as pp:\n",
    "                column=''\n",
    "                second_column=''\n",
    "                \n",
    "                #Let's go through the category column type\n",
    "                for column in categorical_columns:\n",
    "\n",
    "                    #Second index, n^2\n",
    "                    for second_column in ['death_yn']:\n",
    "                        multi_column=[column]\n",
    "\n",
    "\n",
    "                        #Create a key to access - pipe delimited as columns contain _\n",
    "                        grouping_key=\"{}|{}\"\n",
    "\n",
    "                        #No point in grouping the same column twice\n",
    "                        if second_column!=column:\n",
    "                            multi_column+=[second_column]\n",
    "                            grouping_key=grouping_key.format(column,second_column)\n",
    "\n",
    "                            #Separator\n",
    "                            print('\\n\\n----------------------\\n\\n')\n",
    "                            agg_df=df.groupby(multi_column).agg({df.columns[0]:\"count\"})\n",
    "                            agg_df=agg_df.reset_index()\n",
    "                            agg_df=agg_df.rename(columns={df.columns[0]:'Rows'})\n",
    "                            agg_df['% Frequency']=100*(agg_df['Rows']/row_count)\n",
    "\n",
    "                            #Be explicit over what we're displaying\n",
    "                            print('Grouping over {} results in:\\n'.format(grouping_key))\n",
    "\n",
    "                            #Display the result\n",
    "                            display(agg_df)\n",
    "                            agg_df=agg_df.reset_index()\n",
    "\n",
    "                            sagg_df=(\n",
    "                                    df\n",
    "                                    .groupby([column])\n",
    "                                    .agg({df.columns[0]:\"count\"})\n",
    "                                    .reset_index()\n",
    "                                    .rename(columns={df.columns[0]:'TotalRows'})\n",
    "                                    )\n",
    "\n",
    "                            join_df=agg_df.merge(sagg_df,left_on=column,right_on=column,suffixes=('_subbed','_group'))\n",
    "                            join_df['% Stacked']=join_df['Rows']/join_df['TotalRows']\n",
    "\n",
    "                            display(join_df)\n",
    "                            \n",
    "                            if len(join_df)<10000:\n",
    "                                figure=((join_df\n",
    "                                      .pivot_table(index=column,columns=second_column,values='% Stacked')\n",
    "                                      .fillna(0))\n",
    "\n",
    "                                              .plot(kind='bar'\n",
    "                                                , stacked=True\n",
    "                                                , title='Distribution of values for {} vs {}'.format(second_column,column)\n",
    "                                                , xlabel='Field Values'\n",
    "                                                , ylabel='Makeup of Values'\n",
    "                                                , figsize=(35,35)\n",
    "\n",
    "                                ))\n",
    "\n",
    "                                #This grid style is from the sample Lab5 as I like how it looks\n",
    "                                plt.ylim([0,1])\n",
    "                                plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "                                plt.setp(figure.get_xticklabels(), ha=\"right\", rotation=0)\n",
    "                                plt.minorticks_on()\n",
    "                                plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "                                plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    "                                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                                if save_fig:\n",
    "                                    pp.savefig(figure.get_figure())\n",
    "\n",
    "                            #Only save if explicitly passed - This could kill your memory.\n",
    "                            if save_output:\n",
    "                                grouping_type[grouping_key]=agg_df\n",
    "\n",
    "                        \n",
    "    #Catch exceptions\n",
    "    except Exception as exc:\n",
    "        print(\"Function exception:\\n\")\n",
    "        #check exception is memory error\n",
    "        if exc==MemoryError:\n",
    "            print(\"Sorry, your device is not able to run this function as you have hit a memory limit\")\n",
    "            \n",
    "        print(exc)\n",
    "        \n",
    "\n",
    "    return grouping_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_column(df,groupby_columns,agg_dict):\n",
    "    \"\"\"A function to group by columns given and aggregate according to a dictionary.\n",
    "    \n",
    "    Input: df, columns to group by, agg_dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"inside group_by_column(df,{},{})\".format(groupby_columns,agg_dict))\n",
    "    \n",
    "    #Possible Errors\n",
    "    error_dictionary={0:'No Error'\n",
    "                     ,1:'The dataframe is empty'\n",
    "                     ,2:\"The columns to group by is empty or not a list\"\n",
    "                     ,3: 'The dictionary is empty'\n",
    "                     ,4: 'The dataframe does not contain the required columns'\n",
    "                      ,999: 'Uncaught exception'\n",
    "                     }\n",
    "    \n",
    "    #Set as empty\n",
    "    summary_df=pd.DataFrame()\n",
    "    required_columns=[]\n",
    "    \n",
    "    error_code=0\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Dictionary is non-empty\n",
    "        if len(agg_dict)>0 and type(agg_dict)==dict:\n",
    "\n",
    "            #df not empty\n",
    "            if len(df)>0:\n",
    "\n",
    "                #List and non-empty\n",
    "                if type(groupby_columns)==list and len(groupby_columns)>0:\n",
    "                    required_columns=list(df.columns)+list(agg_dict.keys())\n",
    "\n",
    "                    #Required columns found\n",
    "                    if set(required_columns).issubset(set(df.columns)):\n",
    "\n",
    "                        #begin groupby - note: not catching summary issues as they are plentiful\n",
    "                        summary_df=(df\n",
    "                                        .groupby(groupby_columns)\n",
    "                                        .agg(agg_dict)\n",
    "                                        .reset_index()\n",
    "                                    )\n",
    "\n",
    "\n",
    "                    #Required columns not found    \n",
    "                    else:\n",
    "                        error_code=4\n",
    "                        error_message=error_dictionary[error_code]\n",
    "                        print(error_message)\n",
    "\n",
    "                #Not a list or empty\n",
    "                else:\n",
    "                    error_code=2\n",
    "                    error_message=error_dictionary[error_code]\n",
    "                    print(error_message)\n",
    "\n",
    "            #df is not empty\n",
    "            else:\n",
    "                error_code=1\n",
    "                error_message=error_dictionary[error_code]\n",
    "                print(error_message)\n",
    "\n",
    "        #empty Dictionary\n",
    "        else:\n",
    "            error_code=3\n",
    "            error_message=error_dictionary[error_code]\n",
    "            print(error_message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_code=999\n",
    "        print(\"Uncaught exception: {}\".format(e))\n",
    "        \n",
    "    return [error_code,summary_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.02 Begin Looking at the Trips dataset\n",
    "\n",
    "Using Pandas to ingest the dataset and display some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside ingest_data(./data/rt_trips_DB_2018.txt,dictionary)\n",
      "On Chunk: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>TENDERLOT</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>87245</td>\n",
       "      <td>84600</td>\n",
       "      <td>87524</td>\n",
       "      <td>84600</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2967409,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>30517</td>\n",
       "      <td>26460</td>\n",
       "      <td>32752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2580260,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "      <td>35512</td>\n",
       "      <td>32100</td>\n",
       "      <td>36329</td>\n",
       "      <td>32082</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2448968,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6259460</td>\n",
       "      <td>25A</td>\n",
       "      <td>25A_273</td>\n",
       "      <td>1</td>\n",
       "      <td>57261</td>\n",
       "      <td>54420</td>\n",
       "      <td>58463</td>\n",
       "      <td>54443</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,3094242,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253175</td>\n",
       "      <td>14</td>\n",
       "      <td>14_15</td>\n",
       "      <td>1</td>\n",
       "      <td>85383</td>\n",
       "      <td>81600</td>\n",
       "      <td>84682</td>\n",
       "      <td>81608</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2526331,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182632</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765849</td>\n",
       "      <td>123</td>\n",
       "      <td>123_36</td>\n",
       "      <td>2</td>\n",
       "      <td>61560</td>\n",
       "      <td>57840</td>\n",
       "      <td>61365</td>\n",
       "      <td>57859</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,3216350,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182633</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765469</td>\n",
       "      <td>75</td>\n",
       "      <td>75_17</td>\n",
       "      <td>1</td>\n",
       "      <td>53416</td>\n",
       "      <td>48600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48823</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,2865284,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182634</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765486</td>\n",
       "      <td>33D</td>\n",
       "      <td>33D_62</td>\n",
       "      <td>2</td>\n",
       "      <td>29460</td>\n",
       "      <td>26400</td>\n",
       "      <td>29904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,3077688,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182635</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6764987</td>\n",
       "      <td>70</td>\n",
       "      <td>70_60</td>\n",
       "      <td>1</td>\n",
       "      <td>65277</td>\n",
       "      <td>60600</td>\n",
       "      <td>66341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,3208841,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182636</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765012</td>\n",
       "      <td>27</td>\n",
       "      <td>27_19</td>\n",
       "      <td>1</td>\n",
       "      <td>47722</td>\n",
       "      <td>41700</td>\n",
       "      <td>47508</td>\n",
       "      <td>41642</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,2960092,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2182637 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATASOURCE        DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  \\\n",
       "0               DB  07-FEB-18 00:00:00  6253783     68    68_80        1    \n",
       "1               DB  07-FEB-18 00:00:00  6262138    25B  25B_271        2    \n",
       "2               DB  07-FEB-18 00:00:00  6254942    45A   45A_70        2    \n",
       "3               DB  07-FEB-18 00:00:00  6259460    25A  25A_273        1    \n",
       "4               DB  07-FEB-18 00:00:00  6253175     14    14_15        1    \n",
       "...            ...                 ...      ...    ...      ...       ...   \n",
       "2182632         DB  14-MAY-18 00:00:00  6765849    123   123_36        2    \n",
       "2182633         DB  14-MAY-18 00:00:00  6765469     75    75_17        1    \n",
       "2182634         DB  14-MAY-18 00:00:00  6765486    33D   33D_62        2    \n",
       "2182635         DB  14-MAY-18 00:00:00  6764987     70    70_60        1    \n",
       "2182636         DB  14-MAY-18 00:00:00  6765012     27    27_19        1    \n",
       "\n",
       "        PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP   BASIN  \\\n",
       "0                 87245           84600          87524          84600  BasDef   \n",
       "1                 30517           26460          32752            NaN  BasDef   \n",
       "2                 35512           32100          36329          32082  BasDef   \n",
       "3                 57261           54420          58463          54443  BasDef   \n",
       "4                 85383           81600          84682          81608  BasDef   \n",
       "...                 ...             ...            ...            ...     ...   \n",
       "2182632           61560           57840          61365          57859  BasDef   \n",
       "2182633           53416           48600            NaN          48823  BasDef   \n",
       "2182634           29460           26400          29904            NaN  BasDef   \n",
       "2182635           65277           60600          66341            NaN  BasDef   \n",
       "2182636           47722           41700          47508          41642  BasDef   \n",
       "\n",
       "        TENDERLOT SUPPRESSED JUSTIFICATIONID          LASTUPDATE       NOTE  \n",
       "0             NaN        NaN             NaN  28-FEB-18 12:05:11  ,2967409,  \n",
       "1             NaN        NaN             NaN  28-FEB-18 12:05:11  ,2580260,  \n",
       "2             NaN        NaN             NaN  28-FEB-18 12:05:11  ,2448968,  \n",
       "3             NaN        NaN             NaN  28-FEB-18 12:05:11  ,3094242,  \n",
       "4             NaN        NaN             NaN  28-FEB-18 12:05:11  ,2526331,  \n",
       "...           ...        ...             ...                 ...        ...  \n",
       "2182632       NaN        NaN             NaN  26-JUN-18 09:13:13  ,3216350,  \n",
       "2182633       NaN        NaN             NaN  26-JUN-18 09:13:13  ,2865284,  \n",
       "2182634       NaN        NaN             NaN  26-JUN-18 09:13:13  ,3077688,  \n",
       "2182635       NaN        NaN             NaN  26-JUN-18 09:13:13  ,3208841,  \n",
       "2182636       NaN        NaN             NaN  26-JUN-18 09:13:13  ,2960092,  \n",
       "\n",
       "[2182637 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your file contains: \n",
      "2182637 rows x 16 columns.\n",
      "\n",
      "\n",
      "The following columns are present:\n",
      "\"DATASOURCE\"\n",
      "\"DAYOFSERVICE\"\n",
      "\"TRIPID\"\n",
      "\"LINEID\"\n",
      "\"ROUTEID\"\n",
      "\"DIRECTION\"\n",
      "\"PLANNEDTIME_ARR\"\n",
      "\"PLANNEDTIME_DEP\"\n",
      "\"ACTUALTIME_ARR\"\n",
      "\"ACTUALTIME_DEP\"\n",
      "\"BASIN\"\n",
      "\"TENDERLOT\"\n",
      "\"SUPPRESSED\"\n",
      "\"JUSTIFICATIONID\"\n",
      "\"LASTUPDATE\"\n",
      "\"NOTE\"\n",
      "\n",
      "The columns in this data sample match the schema\n",
      "Datetime: NotNumeric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_d3c77_row0_col0,#T_d3c77_row0_col1,#T_d3c77_row0_col2,#T_d3c77_row0_col3,#T_d3c77_row0_col4,#T_d3c77_row0_col5,#T_d3c77_row0_col6,#T_d3c77_row0_col7,#T_d3c77_row0_col8,#T_d3c77_row1_col0,#T_d3c77_row1_col1,#T_d3c77_row1_col2,#T_d3c77_row1_col3,#T_d3c77_row1_col4,#T_d3c77_row1_col5,#T_d3c77_row1_col6,#T_d3c77_row1_col7,#T_d3c77_row1_col8,#T_d3c77_row2_col0,#T_d3c77_row2_col1,#T_d3c77_row2_col2,#T_d3c77_row2_col3,#T_d3c77_row2_col4,#T_d3c77_row2_col5,#T_d3c77_row2_col6,#T_d3c77_row2_col7,#T_d3c77_row2_col8,#T_d3c77_row3_col0,#T_d3c77_row3_col1,#T_d3c77_row3_col2,#T_d3c77_row3_col3,#T_d3c77_row3_col4,#T_d3c77_row3_col5,#T_d3c77_row3_col6,#T_d3c77_row3_col7,#T_d3c77_row3_col8,#T_d3c77_row4_col0,#T_d3c77_row4_col1,#T_d3c77_row4_col2,#T_d3c77_row4_col3,#T_d3c77_row4_col4,#T_d3c77_row4_col5,#T_d3c77_row4_col6,#T_d3c77_row4_col7,#T_d3c77_row4_col8,#T_d3c77_row5_col0,#T_d3c77_row5_col1,#T_d3c77_row5_col2,#T_d3c77_row5_col3,#T_d3c77_row5_col4,#T_d3c77_row5_col5,#T_d3c77_row5_col6,#T_d3c77_row5_col7,#T_d3c77_row5_col8,#T_d3c77_row6_col0,#T_d3c77_row6_col1,#T_d3c77_row6_col2,#T_d3c77_row6_col3,#T_d3c77_row6_col4,#T_d3c77_row6_col5,#T_d3c77_row6_col6,#T_d3c77_row6_col7,#T_d3c77_row6_col8,#T_d3c77_row7_col0,#T_d3c77_row7_col1,#T_d3c77_row7_col2,#T_d3c77_row7_col3,#T_d3c77_row7_col4,#T_d3c77_row7_col5,#T_d3c77_row7_col6,#T_d3c77_row7_col7,#T_d3c77_row7_col8,#T_d3c77_row10_col0,#T_d3c77_row10_col1,#T_d3c77_row10_col2,#T_d3c77_row10_col3,#T_d3c77_row10_col4,#T_d3c77_row10_col5,#T_d3c77_row10_col6,#T_d3c77_row10_col7,#T_d3c77_row10_col8,#T_d3c77_row14_col0,#T_d3c77_row14_col1,#T_d3c77_row14_col2,#T_d3c77_row14_col3,#T_d3c77_row14_col4,#T_d3c77_row14_col5,#T_d3c77_row14_col6,#T_d3c77_row14_col7,#T_d3c77_row14_col8,#T_d3c77_row15_col0,#T_d3c77_row15_col1,#T_d3c77_row15_col2,#T_d3c77_row15_col3,#T_d3c77_row15_col4,#T_d3c77_row15_col5,#T_d3c77_row15_col6,#T_d3c77_row15_col7,#T_d3c77_row15_col8{\n",
       "            background-color:  white;\n",
       "        }#T_d3c77_row8_col0,#T_d3c77_row8_col1,#T_d3c77_row8_col2,#T_d3c77_row8_col3,#T_d3c77_row8_col4,#T_d3c77_row8_col5,#T_d3c77_row8_col6,#T_d3c77_row8_col7,#T_d3c77_row8_col8,#T_d3c77_row9_col0,#T_d3c77_row9_col1,#T_d3c77_row9_col2,#T_d3c77_row9_col3,#T_d3c77_row9_col4,#T_d3c77_row9_col5,#T_d3c77_row9_col6,#T_d3c77_row9_col7,#T_d3c77_row9_col8{\n",
       "            background-color:  yellow;\n",
       "        }#T_d3c77_row11_col0,#T_d3c77_row11_col1,#T_d3c77_row11_col2,#T_d3c77_row11_col3,#T_d3c77_row11_col4,#T_d3c77_row11_col5,#T_d3c77_row11_col6,#T_d3c77_row11_col7,#T_d3c77_row11_col8,#T_d3c77_row12_col0,#T_d3c77_row12_col1,#T_d3c77_row12_col2,#T_d3c77_row12_col3,#T_d3c77_row12_col4,#T_d3c77_row12_col5,#T_d3c77_row12_col6,#T_d3c77_row12_col7,#T_d3c77_row12_col8,#T_d3c77_row13_col0,#T_d3c77_row13_col1,#T_d3c77_row13_col2,#T_d3c77_row13_col3,#T_d3c77_row13_col4,#T_d3c77_row13_col5,#T_d3c77_row13_col6,#T_d3c77_row13_col7,#T_d3c77_row13_col8{\n",
       "            background-color:  red;\n",
       "        }</style><table id=\"T_d3c77_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >feature</th>        <th class=\"col_heading level0 col1\" >count</th>        <th class=\"col_heading level0 col2\" >unique</th>        <th class=\"col_heading level0 col3\" >top</th>        <th class=\"col_heading level0 col4\" >freq</th>        <th class=\"col_heading level0 col5\" >% Populated</th>        <th class=\"col_heading level0 col6\" >% Missing</th>        <th class=\"col_heading level0 col7\" >% Top Value</th>        <th class=\"col_heading level0 col8\" >Missing Warning</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d3c77_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_d3c77_row0_col0\" class=\"data row0 col0\" >DATASOURCE</td>\n",
       "                        <td id=\"T_d3c77_row0_col1\" class=\"data row0 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "                        <td id=\"T_d3c77_row0_col3\" class=\"data row0 col3\" >DB</td>\n",
       "                        <td id=\"T_d3c77_row0_col4\" class=\"data row0 col4\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row0_col5\" class=\"data row0 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row0_col6\" class=\"data row0 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row0_col7\" class=\"data row0 col7\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row0_col8\" class=\"data row0 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_d3c77_row1_col0\" class=\"data row1 col0\" >DAYOFSERVICE</td>\n",
       "                        <td id=\"T_d3c77_row1_col1\" class=\"data row1 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row1_col2\" class=\"data row1 col2\" >360</td>\n",
       "                        <td id=\"T_d3c77_row1_col3\" class=\"data row1 col3\" >12-FEB-18 00:00:00</td>\n",
       "                        <td id=\"T_d3c77_row1_col4\" class=\"data row1 col4\" >7122</td>\n",
       "                        <td id=\"T_d3c77_row1_col5\" class=\"data row1 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row1_col6\" class=\"data row1 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row1_col7\" class=\"data row1 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row1_col8\" class=\"data row1 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_d3c77_row2_col0\" class=\"data row2 col0\" >TRIPID</td>\n",
       "                        <td id=\"T_d3c77_row2_col1\" class=\"data row2 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row2_col2\" class=\"data row2 col2\" >658964</td>\n",
       "                        <td id=\"T_d3c77_row2_col3\" class=\"data row2 col3\" >7319095</td>\n",
       "                        <td id=\"T_d3c77_row2_col4\" class=\"data row2 col4\" >19</td>\n",
       "                        <td id=\"T_d3c77_row2_col5\" class=\"data row2 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row2_col6\" class=\"data row2 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row2_col7\" class=\"data row2 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row2_col8\" class=\"data row2 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_d3c77_row3_col0\" class=\"data row3 col0\" >LINEID</td>\n",
       "                        <td id=\"T_d3c77_row3_col1\" class=\"data row3 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row3_col2\" class=\"data row3 col2\" >130</td>\n",
       "                        <td id=\"T_d3c77_row3_col3\" class=\"data row3 col3\" >46A</td>\n",
       "                        <td id=\"T_d3c77_row3_col4\" class=\"data row3 col4\" >76728</td>\n",
       "                        <td id=\"T_d3c77_row3_col5\" class=\"data row3 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row3_col6\" class=\"data row3 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row3_col7\" class=\"data row3 col7\" >4%</td>\n",
       "                        <td id=\"T_d3c77_row3_col8\" class=\"data row3 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_d3c77_row4_col0\" class=\"data row4 col0\" >ROUTEID</td>\n",
       "                        <td id=\"T_d3c77_row4_col1\" class=\"data row4 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row4_col2\" class=\"data row4 col2\" >588</td>\n",
       "                        <td id=\"T_d3c77_row4_col3\" class=\"data row4 col3\" >46A_74</td>\n",
       "                        <td id=\"T_d3c77_row4_col4\" class=\"data row4 col4\" >37182</td>\n",
       "                        <td id=\"T_d3c77_row4_col5\" class=\"data row4 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row4_col6\" class=\"data row4 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row4_col7\" class=\"data row4 col7\" >2%</td>\n",
       "                        <td id=\"T_d3c77_row4_col8\" class=\"data row4 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_d3c77_row5_col0\" class=\"data row5 col0\" >DIRECTION</td>\n",
       "                        <td id=\"T_d3c77_row5_col1\" class=\"data row5 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row5_col2\" class=\"data row5 col2\" >2</td>\n",
       "                        <td id=\"T_d3c77_row5_col3\" class=\"data row5 col3\" >2 </td>\n",
       "                        <td id=\"T_d3c77_row5_col4\" class=\"data row5 col4\" >1100273</td>\n",
       "                        <td id=\"T_d3c77_row5_col5\" class=\"data row5 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row5_col6\" class=\"data row5 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row5_col7\" class=\"data row5 col7\" >50%</td>\n",
       "                        <td id=\"T_d3c77_row5_col8\" class=\"data row5 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_d3c77_row6_col0\" class=\"data row6 col0\" >PLANNEDTIME_ARR</td>\n",
       "                        <td id=\"T_d3c77_row6_col1\" class=\"data row6 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row6_col2\" class=\"data row6 col2\" >64461</td>\n",
       "                        <td id=\"T_d3c77_row6_col3\" class=\"data row6 col3\" >31620</td>\n",
       "                        <td id=\"T_d3c77_row6_col4\" class=\"data row6 col4\" >467</td>\n",
       "                        <td id=\"T_d3c77_row6_col5\" class=\"data row6 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row6_col6\" class=\"data row6 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row6_col7\" class=\"data row6 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row6_col8\" class=\"data row6 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_d3c77_row7_col0\" class=\"data row7 col0\" >PLANNEDTIME_DEP</td>\n",
       "                        <td id=\"T_d3c77_row7_col1\" class=\"data row7 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row7_col2\" class=\"data row7 col2\" >791</td>\n",
       "                        <td id=\"T_d3c77_row7_col3\" class=\"data row7 col3\" >61200</td>\n",
       "                        <td id=\"T_d3c77_row7_col4\" class=\"data row7 col4\" >26879</td>\n",
       "                        <td id=\"T_d3c77_row7_col5\" class=\"data row7 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row7_col6\" class=\"data row7 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row7_col7\" class=\"data row7 col7\" >1%</td>\n",
       "                        <td id=\"T_d3c77_row7_col8\" class=\"data row7 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_d3c77_row8_col0\" class=\"data row8 col0\" >ACTUALTIME_ARR</td>\n",
       "                        <td id=\"T_d3c77_row8_col1\" class=\"data row8 col1\" >2045430</td>\n",
       "                        <td id=\"T_d3c77_row8_col2\" class=\"data row8 col2\" >68122</td>\n",
       "                        <td id=\"T_d3c77_row8_col3\" class=\"data row8 col3\" >65646</td>\n",
       "                        <td id=\"T_d3c77_row8_col4\" class=\"data row8 col4\" >69</td>\n",
       "                        <td id=\"T_d3c77_row8_col5\" class=\"data row8 col5\" >94%</td>\n",
       "                        <td id=\"T_d3c77_row8_col6\" class=\"data row8 col6\" >6%</td>\n",
       "                        <td id=\"T_d3c77_row8_col7\" class=\"data row8 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row8_col8\" class=\"data row8 col8\" >Low</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_d3c77_row9_col0\" class=\"data row9 col0\" >ACTUALTIME_DEP</td>\n",
       "                        <td id=\"T_d3c77_row9_col1\" class=\"data row9 col1\" >2018086</td>\n",
       "                        <td id=\"T_d3c77_row9_col2\" class=\"data row9 col2\" >66771</td>\n",
       "                        <td id=\"T_d3c77_row9_col3\" class=\"data row9 col3\" >84607</td>\n",
       "                        <td id=\"T_d3c77_row9_col4\" class=\"data row9 col4\" >458</td>\n",
       "                        <td id=\"T_d3c77_row9_col5\" class=\"data row9 col5\" >92%</td>\n",
       "                        <td id=\"T_d3c77_row9_col6\" class=\"data row9 col6\" >8%</td>\n",
       "                        <td id=\"T_d3c77_row9_col7\" class=\"data row9 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row9_col8\" class=\"data row9 col8\" >Low</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_d3c77_row10_col0\" class=\"data row10 col0\" >BASIN</td>\n",
       "                        <td id=\"T_d3c77_row10_col1\" class=\"data row10 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row10_col2\" class=\"data row10 col2\" >1</td>\n",
       "                        <td id=\"T_d3c77_row10_col3\" class=\"data row10 col3\" >BasDef</td>\n",
       "                        <td id=\"T_d3c77_row10_col4\" class=\"data row10 col4\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row10_col5\" class=\"data row10 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row10_col6\" class=\"data row10 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row10_col7\" class=\"data row10 col7\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row10_col8\" class=\"data row10 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_d3c77_row11_col0\" class=\"data row11 col0\" >TENDERLOT</td>\n",
       "                        <td id=\"T_d3c77_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "                        <td id=\"T_d3c77_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "                        <td id=\"T_d3c77_row11_col3\" class=\"data row11 col3\" >nan</td>\n",
       "                        <td id=\"T_d3c77_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
       "                        <td id=\"T_d3c77_row11_col5\" class=\"data row11 col5\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row11_col6\" class=\"data row11 col6\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row11_col7\" class=\"data row11 col7\" >nan%</td>\n",
       "                        <td id=\"T_d3c77_row11_col8\" class=\"data row11 col8\" >High</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_d3c77_row12_col0\" class=\"data row12 col0\" >SUPPRESSED</td>\n",
       "                        <td id=\"T_d3c77_row12_col1\" class=\"data row12 col1\" >4333</td>\n",
       "                        <td id=\"T_d3c77_row12_col2\" class=\"data row12 col2\" >1</td>\n",
       "                        <td id=\"T_d3c77_row12_col3\" class=\"data row12 col3\" >0</td>\n",
       "                        <td id=\"T_d3c77_row12_col4\" class=\"data row12 col4\" >4333</td>\n",
       "                        <td id=\"T_d3c77_row12_col5\" class=\"data row12 col5\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row12_col6\" class=\"data row12 col6\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row12_col7\" class=\"data row12 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row12_col8\" class=\"data row12 col8\" >High</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_d3c77_row13_col0\" class=\"data row13 col0\" >JUSTIFICATIONID</td>\n",
       "                        <td id=\"T_d3c77_row13_col1\" class=\"data row13 col1\" >4330</td>\n",
       "                        <td id=\"T_d3c77_row13_col2\" class=\"data row13 col2\" >3526</td>\n",
       "                        <td id=\"T_d3c77_row13_col3\" class=\"data row13 col3\" >203708</td>\n",
       "                        <td id=\"T_d3c77_row13_col4\" class=\"data row13 col4\" >19</td>\n",
       "                        <td id=\"T_d3c77_row13_col5\" class=\"data row13 col5\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row13_col6\" class=\"data row13 col6\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row13_col7\" class=\"data row13 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row13_col8\" class=\"data row13 col8\" >High</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_d3c77_row14_col0\" class=\"data row14 col0\" >LASTUPDATE</td>\n",
       "                        <td id=\"T_d3c77_row14_col1\" class=\"data row14 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row14_col2\" class=\"data row14 col2\" >360</td>\n",
       "                        <td id=\"T_d3c77_row14_col3\" class=\"data row14 col3\" >28-FEB-18 13:18:29</td>\n",
       "                        <td id=\"T_d3c77_row14_col4\" class=\"data row14 col4\" >7122</td>\n",
       "                        <td id=\"T_d3c77_row14_col5\" class=\"data row14 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row14_col6\" class=\"data row14 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row14_col7\" class=\"data row14 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row14_col8\" class=\"data row14 col8\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d3c77_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_d3c77_row15_col0\" class=\"data row15 col0\" >NOTE</td>\n",
       "                        <td id=\"T_d3c77_row15_col1\" class=\"data row15 col1\" >2182637</td>\n",
       "                        <td id=\"T_d3c77_row15_col2\" class=\"data row15 col2\" >46690</td>\n",
       "                        <td id=\"T_d3c77_row15_col3\" class=\"data row15 col3\" >,2668673,</td>\n",
       "                        <td id=\"T_d3c77_row15_col4\" class=\"data row15 col4\" >244</td>\n",
       "                        <td id=\"T_d3c77_row15_col5\" class=\"data row15 col5\" >100%</td>\n",
       "                        <td id=\"T_d3c77_row15_col6\" class=\"data row15 col6\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row15_col7\" class=\"data row15 col7\" >0%</td>\n",
       "                        <td id=\"T_d3c77_row15_col8\" class=\"data row15 col8\" >None</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1b9c002a00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Sample Data:\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>TENDERLOT</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>87245</td>\n",
       "      <td>84600</td>\n",
       "      <td>87524</td>\n",
       "      <td>84600</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2967409,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>30517</td>\n",
       "      <td>26460</td>\n",
       "      <td>32752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2580260,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "      <td>35512</td>\n",
       "      <td>32100</td>\n",
       "      <td>36329</td>\n",
       "      <td>32082</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2448968,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6259460</td>\n",
       "      <td>25A</td>\n",
       "      <td>25A_273</td>\n",
       "      <td>1</td>\n",
       "      <td>57261</td>\n",
       "      <td>54420</td>\n",
       "      <td>58463</td>\n",
       "      <td>54443</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,3094242,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253175</td>\n",
       "      <td>14</td>\n",
       "      <td>14_15</td>\n",
       "      <td>1</td>\n",
       "      <td>85383</td>\n",
       "      <td>81600</td>\n",
       "      <td>84682</td>\n",
       "      <td>81608</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2526331,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  \\\n",
       "0         DB  07-FEB-18 00:00:00  6253783     68    68_80        1    \n",
       "1         DB  07-FEB-18 00:00:00  6262138    25B  25B_271        2    \n",
       "2         DB  07-FEB-18 00:00:00  6254942    45A   45A_70        2    \n",
       "3         DB  07-FEB-18 00:00:00  6259460    25A  25A_273        1    \n",
       "4         DB  07-FEB-18 00:00:00  6253175     14    14_15        1    \n",
       "\n",
       "  PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP   BASIN  \\\n",
       "0           87245           84600          87524          84600  BasDef   \n",
       "1           30517           26460          32752            NaN  BasDef   \n",
       "2           35512           32100          36329          32082  BasDef   \n",
       "3           57261           54420          58463          54443  BasDef   \n",
       "4           85383           81600          84682          81608  BasDef   \n",
       "\n",
       "  TENDERLOT SUPPRESSED JUSTIFICATIONID          LASTUPDATE       NOTE  \n",
       "0       NaN        NaN             NaN  28-FEB-18 12:05:11  ,2967409,  \n",
       "1       NaN        NaN             NaN  28-FEB-18 12:05:11  ,2580260,  \n",
       "2       NaN        NaN             NaN  28-FEB-18 12:05:11  ,2448968,  \n",
       "3       NaN        NaN             NaN  28-FEB-18 12:05:11  ,3094242,  \n",
       "4       NaN        NaN             NaN  28-FEB-18 12:05:11  ,2526331,  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bus_trips_df=ingest_data(fp=bus_trips_filepath\n",
    "                ,delim=bus_trips_sep\n",
    "                ,data_dictionary=bus_trips_data_dictionary\n",
    "                ,chunks=10000000\n",
    "                ,pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside data_convert()\n",
      "Converting to datetime\n",
      "Inside data_convert()\n",
      "Converting to category\n",
      "Inside data_convert()\n",
      "Converting to Numerical\n"
     ]
    }
   ],
   "source": [
    "data_convert(bus_trips_df,'datetime',tp_datetime_columns,datetime_format)\n",
    "data_convert(bus_trips_df,'category',tp_categorical_columns,datetime_format)\n",
    "data_convert(bus_trips_df,'numeric',tp_num_columns,datetime_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_over_single_categories(bus_trips_df,categorical_columns=tp_categorical_columns,pdf_fn=\"./data/rt_trips_single_DB_{}.pdf\".format(today_date), save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_over_multi_categories(bus_trips_df,categorical_columns=tp_categorical_columns,pdf_fn=\"./data/rt_multi_trips_DB_{}.pdf\".format(today_date), save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.02 Looking at the Leavetime dataset\n",
    "\n",
    "I will use dask to look at the dataset as it can load more things and is supposed to be faster for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside ingest_data(./data/rt_leavetimes_DB_2018.txt,dictionary)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>PASSENGERS</th>\n",
       "      <th>PASSENGERSIN</th>\n",
       "      <th>PASSENGERSOUT</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=337</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 337 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                DATASOURCE DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP VEHICLEID PASSENGERS PASSENGERSIN PASSENGERSOUT DISTANCE SUPPRESSED JUSTIFICATIONID LASTUPDATE     NOTE\n",
       "npartitions=337                                                                                                                                                                                                                             \n",
       "                    object       object  int64       int64       int64           int64           int64          int64          int64     int64    float64      float64       float64  float64    float64         float64     object  float64\n",
       "                       ...          ...    ...         ...         ...             ...             ...            ...            ...       ...        ...          ...           ...      ...        ...             ...        ...      ...\n",
       "...                    ...          ...    ...         ...         ...             ...             ...            ...            ...       ...        ...          ...           ...      ...        ...             ...        ...      ...\n",
       "                       ...          ...    ...         ...         ...             ...             ...            ...            ...       ...        ...          ...           ...      ...        ...             ...        ...      ...\n",
       "                       ...          ...    ...         ...         ...             ...             ...            ...            ...       ...        ...          ...           ...      ...        ...             ...        ...      ...\n",
       "Dask Name: read-csv, 337 tasks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your file contains: \n",
      "Delayed('int-28d3a588-2053-4943-9a62-69b6f65f0dbd') rows x 18 columns.\n",
      "\n",
      "\n",
      "The following columns are present:\n",
      "\"DATASOURCE\"\n",
      "\"DAYOFSERVICE\"\n",
      "\"TRIPID\"\n",
      "\"PROGRNUMBER\"\n",
      "\"STOPPOINTID\"\n",
      "\"PLANNEDTIME_ARR\"\n",
      "\"PLANNEDTIME_DEP\"\n",
      "\"ACTUALTIME_ARR\"\n",
      "\"ACTUALTIME_DEP\"\n",
      "\"VEHICLEID\"\n",
      "\"PASSENGERS\"\n",
      "\"PASSENGERSIN\"\n",
      "\"PASSENGERSOUT\"\n",
      "\"DISTANCE\"\n",
      "\"SUPPRESSED\"\n",
      "\"JUSTIFICATIONID\"\n",
      "\"LASTUPDATE\"\n",
      "\"NOTE\"\n",
      "\n",
      "The columns in this data sample match the schema\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No non-trivial arrays found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-52837ab9c2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bus_leavetimes_df=ingest_data(fp=bus_leavetimes_filepath\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mdelim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbus_leavetimes_sep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mdata_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbus_leavetimes_data_dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 ,pandas=False)\n",
      "\u001b[0;32m<ipython-input-34-7139b36e8b05>\u001b[0m in \u001b[0;36mingest_data\u001b[0;34m(fp, delim, data_dictionary, chunks, pandas)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_datetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_datetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-7139b36e8b05>\u001b[0m in \u001b[0;36mdescriptive_stats\u001b[0;34m(df, num_datetime, pandas)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mcategory_summary_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsk_describe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdask_df_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m#Add what Percent is populated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-4e00f95ba07a>\u001b[0m in \u001b[0;36mdsk_describe\u001b[0;34m(dask_df_iter)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdesc_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdask_df_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdesc_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m                             \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re-execute locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m                             \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m                     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/core.py\u001b[0m in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# temporaries by their reference count and can execute certain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# operations in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/core.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# temporaries by their reference count and can execute certain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# operations in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/core.py\u001b[0m in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# temporaries by their reference count and can execute certain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# operations in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/dask/array/percentile.py\u001b[0m in \u001b[0;36mmerge_percentiles\u001b[0;34m(finalq, qs, vals, interpolation, Ns)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No non-trivial arrays found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No non-trivial arrays found"
     ]
    }
   ],
   "source": [
    "bus_leavetimes_df=ingest_data(fp=bus_leavetimes_filepath\n",
    "                ,delim=bus_leavetimes_sep\n",
    "                ,data_dictionary=bus_leavetimes_data_dictionary\n",
    "                ,chunks=10000000\n",
    "                ,pandas=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the datatypes of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bus_leavetimes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5b813807ecfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbus_leavetimes_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp_datetime_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbus_leavetimes_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp_categorical_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbus_leavetimes_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'numeric'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp_num_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bus_leavetimes_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_convert(bus_leavetimes_df,'datetime',tp_datetime_columns,datetime_format)\n",
    "data_convert(bus_leavetimes_df,'category',tp_categorical_columns,datetime_format)\n",
    "data_convert(bus_leavetimes_df,'numeric',tp_num_columns,datetime_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform basic tasks to look at the dataset as a whole, so that we can look at the dataset as a whole\n",
    "\n",
    "Looking for the max value and the min value will show us if there are extreme anomilies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 30s, sys: 45.4 s, total: 7min 15s\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DATASOURCE                           DB\n",
       "DAYOFSERVICE         31-OCT-18 00:00:00\n",
       "TRIPID                          8592207\n",
       "PROGRNUMBER                         109\n",
       "STOPPOINTID                        7692\n",
       "PLANNEDTIME_ARR                   91680\n",
       "PLANNEDTIME_DEP                   91680\n",
       "ACTUALTIME_ARR                    97177\n",
       "ACTUALTIME_DEP                    97177\n",
       "VEHICLEID                       3394131\n",
       "PASSENGERS                         None\n",
       "PASSENGERSIN                       None\n",
       "PASSENGERSOUT                      None\n",
       "DISTANCE                           None\n",
       "SUPPRESSED                          1.0\n",
       "JUSTIFICATIONID    484981386680422208.0\n",
       "LASTUPDATE           31-JAN-18 21:17:42\n",
       "NOTE                               None\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max values of the dataset\n",
    "%time bus_leavetimes_df.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum values of the dataset\n",
    "%time bus_leavetimes_df.min().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    object\n",
       "       ...\n",
       "Name: DATASOURCE, dtype: object\n",
       "Dask Name: unique-agg, 1061 tasks"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_leavetimes_df['DATASOURCE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "'DATASOURCE':'category'\n",
      "'DAYOFSERVICE':'object'\n",
      "'TRIPID':'category'\n",
      "'LINEID':'category'\n",
      "'ROUTEID':'category'\n",
      "'DIRECTION':'category'\n",
      "'PLANNEDTIME_ARR':'int64'\n",
      "'PLANNEDTIME_DEP':'int64'\n",
      "'ACTUALTIME_ARR':'float64'\n",
      "'ACTUALTIME_DEP':'float64'\n",
      "'BASIN':'category'\n",
      "'TENDERLOT':'float64'\n",
      "'SUPPRESSED':'category'\n",
      "'JUSTIFICATIONID':'category'\n",
      "'LASTUPDATE':'object'\n",
      "'NOTE':'object'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_file_dict={'Data':bus_trips_df}\n",
    "\n",
    "for key, value in static_file_dict.items():\n",
    "    print(key)\n",
    "    all_print=''\n",
    "    print_statement=\"'{}':'{}'\\n\"\n",
    "    \n",
    "    for rkey, rvalue in value.dtypes.apply(lambda x: x.name).to_dict().items():\n",
    "        all_print+=print_statement.format(rkey,rvalue)\n",
    "    \n",
    "    print(all_print)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in ./miniconda3/lib/python3.9/site-packages (2021.6.2)\n",
      "Requirement already satisfied: pyyaml in ./miniconda3/lib/python3.9/site-packages (from dask) (5.4.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in ./miniconda3/lib/python3.9/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in ./miniconda3/lib/python3.9/site-packages (from dask) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in ./miniconda3/lib/python3.9/site-packages (from dask) (2021.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in ./miniconda3/lib/python3.9/site-packages (from dask) (1.6.0)\n",
      "Requirement already satisfied: locket in ./miniconda3/lib/python3.9/site-packages (from partd>=0.3.10->dask) (0.2.1)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.9/site-packages (2021.6.1)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: fsspec==2021.06.1 in ./miniconda3/lib/python3.9/site-packages (from s3fs) (2021.6.1)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.20.50,>=1.20.49\n",
      "  Downloading botocore-1.20.49-py2.py3-none-any.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp>=3.3.1\n",
      "  Downloading aiohttp-3.7.4.post0-cp39-cp39-manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 46.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.10.10\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.7.1-py3-none-any.whl (20 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in ./miniconda3/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.10.0.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in ./miniconda3/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (4.0.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp39-cp39-manylinux2014_x86_64.whl (315 kB)\n",
      "\u001b[K     |████████████████████████████████| 315 kB 55.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (20.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp39-cp39-manylinux2014_x86_64.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 43.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./miniconda3/lib/python3.9/site-packages (from botocore<1.20.50,>=1.20.49->aiobotocore>=1.0.1->s3fs) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./miniconda3/lib/python3.9/site-packages (from botocore<1.20.50,>=1.20.49->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.50,>=1.20.49->aiobotocore>=1.0.1->s3fs) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.10)\n",
      "Building wheels for collected packages: aiobotocore, wrapt\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.3.1-py3-none-any.whl size=46610 sha256=7c6934ff123cc1990be1cd951c5eb5420b8cff1404ed3195d78dfdeda48a3ce2\n",
      "  Stored in directory: /home/team10/.cache/pip/wheels/d4/cc/53/c59c63c30d6803eb40ab981f7d596b7386ca3ad06324ae1960\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=36884 sha256=9fffff98fb481af9499c0db3aada1979d301fe42a8bb94dfd1fce29a129769a6\n",
      "  Stored in directory: /home/team10/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
      "Successfully built aiobotocore wrapt\n",
      "Installing collected packages: multidict, yarl, jmespath, async-timeout, wrapt, botocore, aioitertools, aiohttp, aiobotocore, s3fs\n",
      "Successfully installed aiobotocore-1.3.1 aiohttp-3.7.4.post0 aioitertools-0.7.1 async-timeout-3.0.1 botocore-1.20.49 jmespath-0.10.0 multidict-5.1.0 s3fs-2021.6.1 wrapt-1.12.1 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install --upgrade dask\n",
    "! python -m pip install fsspec\n",
    "! python -m pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
