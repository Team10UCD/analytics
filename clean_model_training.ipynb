{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data of a dask directory and converting into a pandas dataframe\n",
    "def read_dd_to_pandas(route):\n",
    "    route_dask_df = dd.read_csv(\"~/data/leavetimes_split_by_route/{}/*.part\".format(route))\n",
    "    route_dask_df = route_dask_df.drop('Unnamed: 0', axis=1)\n",
    "    route_pd_df = route_dask_df.compute()\n",
    "    route_pd_df = route_pd_df.reset_index(drop=True)\n",
    "    # changing the datatypes\n",
    "    route_pd_df[\"DAYOFSERVICE\"] = pd.to_datetime(route_pd_df[\"DAYOFSERVICE\"])\n",
    "    route_pd_df[\"LASTUPDATE\"] = pd.to_datetime(route_pd_df[\"LASTUPDATE\"])\n",
    "\n",
    "    # categorical things\n",
    "    route_pd_df[\"TRIPID\"] = route_pd_df[\"TRIPID\"].astype(\"category\")\n",
    "    route_pd_df[\"VEHICLEID\"] = route_pd_df[\"VEHICLEID\"].astype(\"category\")\n",
    "    route_pd_df[\"SUPPRESSED\"] = route_pd_df[\"SUPPRESSED\"].astype(\"category\")\n",
    "    route_pd_df[\"STOPPOINTID\"] = route_pd_df[\"STOPPOINTID\"].astype(\"category\")\n",
    "    #route_pd_df[\"PROGRNUMBER\"] = route_pd_df[\"PROGRNUMBER\"].astype(\"category\")\n",
    "    \n",
    "    # NUMBERICAL tings\n",
    "    # all other features, will keep these the same as before\n",
    "    # drop the features to save memory\n",
    "    route_pd_df = route_pd_df.drop([\"LASTUPDATE\", \"VEHICLEID\", \"PLANNEDTIME_DEP\", \"ACTUALTIME_DEP\", \"SUPPRESSED\", \"JUSTIFICATIONID\"], axis = 1)\n",
    "    return route_pd_df\n",
    "\n",
    "def holiday(df):\n",
    "    holiday = [6,7,8, 12]\n",
    "    if df[\"DAYOFSERVICE\"].month in holiday:\n",
    "        return 1\n",
    "    elif df[\"DAYOFSERVICE\"].month == 3 and df[\"DAYOFSERVICE\"].day == 17:\n",
    "        return 1\n",
    "    elif df[\"DAYOFSERVICE\"].month == 1 and df[\"DAYOFSERVICE\"].day == 1:\n",
    "        return 1\n",
    "    elif df[\"DAYOFSERVICE\"].month == 5 and df[\"DAYOFSERVICE\"].day == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# create mapping function\n",
    "def seconds_to_time(n):\n",
    "    return timedelta(seconds=n)\n",
    "\n",
    "# creating new features\n",
    "def create_features(route_pd_df):\n",
    "    # we need to merge the data for the route and the weather data\n",
    "    #merge with respect to the time, and eazch hour\n",
    "    # for example, 1:30 should be emrged with the data for 1:00 as weather\n",
    "    # is on hourly basis\n",
    "\n",
    "    # CREATE  a new feature in the routes that contains the hours so that we can \n",
    "    # merge with the weather data\n",
    "    route_time_planned = route_pd_df[\"PLANNEDTIME_ARR\"]\n",
    "    route_time_actual = route_pd_df[\"ACTUALTIME_ARR\"]\n",
    "\n",
    "    actual_time_hour = []\n",
    "    expected_time_hour = []\n",
    "    for i in range(len(route_time_actual)):\n",
    "        actual_time_hour.append(route_time_actual[i] // 3600)\n",
    "        expected_time_hour.append(route_time_planned[i] // 3600)\n",
    "\n",
    "    actual_date = route_pd_df[\"DAYOFSERVICE\"]\n",
    "\n",
    "\n",
    "    # parse it into datetime with this  any hours\n",
    "    date_time = []\n",
    "    for i in range(len(actual_time_hour)):\n",
    "        new_datetime = actual_date[i] + timedelta(hours = int(actual_time_hour[i]))\n",
    "        date_time.append(new_datetime)\n",
    "\n",
    "    # add in the new column, \n",
    "    # new column means that we can remove the other date column\n",
    "    route_pd_df[\"date_and_time\"] = date_time\n",
    "\n",
    "    route_pd_df[\"delay_amount\"] = route_pd_df[\"ACTUALTIME_ARR\"] - route_pd_df[\"PLANNEDTIME_ARR\"]\n",
    "\n",
    "    #combined_df[\"planned_hour_arr\"]\n",
    "    hours = route_pd_df[\"PLANNEDTIME_ARR\"]//3600\n",
    "    #combined_df[\"planned_minute_arr\"]\n",
    "\n",
    "    minutes= (route_pd_df[\"PLANNEDTIME_ARR\"]%3600) // 60\n",
    "\n",
    "    route_pd_df[\"planned_arr_hours\"] = hours\n",
    "    route_pd_df[\"planned_arr_minutes\"] = minutes\n",
    "    # get the day of the week form the \n",
    "    route_pd_df[\"day_of_week\"] = route_pd_df[\"DAYOFSERVICE\"].dt.day_name()\n",
    "    route_pd_df[\"holiday\"] = route_pd_df.apply(holiday, axis = 1)\n",
    "    route_pd_df[ \"datetime_exact\"] = route_pd_df[\"DAYOFSERVICE\"] + pd.Series(map(seconds_to_time, route_pd_df[\"PLANNEDTIME_ARR\"]), name=\"time\")\n",
    "\n",
    "    return route_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the trips data\n",
    "trips_df = dd.read_csv(\"data/rt_trips_DB_2018.txt\", sep = \";\")\n",
    "trips_df= trips_df[[\"TRIPID\", \"DIRECTION\"]].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASOURCE              1\n",
       "DAYOFSERVICE          360\n",
       "TRIPID             658964\n",
       "LINEID                130\n",
       "ROUTEID               588\n",
       "DIRECTION               2\n",
       "PLANNEDTIME_ARR     64461\n",
       "PLANNEDTIME_DEP       791\n",
       "ACTUALTIME_ARR      68122\n",
       "ACTUALTIME_DEP      66771\n",
       "BASIN                   1\n",
       "TENDERLOT               0\n",
       "SUPPRESSED              1\n",
       "JUSTIFICATIONID      3526\n",
       "LASTUPDATE            360\n",
       "NOTE                46690\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_df = dd.read_csv(\"data/rt_trips_DB_2018.txt\", sep = \";\")\n",
    "trips_df.compute().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRIPID       658964\n",
       "DIRECTION         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658964, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the \n",
    "trips_df.to_csv(\"data/trips_extract.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoders from disk\n",
    "loaded_encoder = joblib.load(\"models/days_of_week_one_hot_encoder.sav\")\n",
    "weather_encoder = joblib.load(\"models/weather_encoder.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read weather data\n",
    "weather_df = pd.read_csv(\"~/data/openweatherapi_2018_data_with_columns_removed2021-06-26 01:55:47.555341.csv\")\n",
    "\n",
    "# change the dtypes of the weather features\n",
    "weather_df[\"dt\"] = pd.to_datetime(weather_df[\"dt\"]).dt.tz_localize(None)\n",
    "\n",
    "# categorical data\n",
    "weather_df[\"weather_main\"] = weather_df[\"weather_main\"].astype(\"category\")\n",
    "weather_df[\"weather_icon\"] = weather_df[\"weather_icon\"].astype(\"category\")\n",
    "# reduce the amount of things stored in memory\n",
    "weather_df = weather_df[[\"dt\", \"humidity\", \"rain_1h\", \"weather_main\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 14/130 [42:32<7:38:04, 236.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route39A had an error with memory. train locally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 27/130 [1:11:31<6:20:43, 221.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route145 had an error with memory. train locally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [4:54:58<00:00, 136.14s/it]  \n"
     ]
    }
   ],
   "source": [
    "directory = 'data/leavetimes_split_by_route'\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    try:\n",
    "        route = filename\n",
    "\n",
    "        # reading the dataframe\n",
    "        route_pd_df = read_dd_to_pandas(route)\n",
    "        # add new features\n",
    "        route_pd_df = create_features(route_pd_df)\n",
    "        # create different features\n",
    "    #     route_pd_df[\"holiday\"] = route_pd_df.apply(holiday, axis = 1)\n",
    "    #     route_pd_df[ \"datetime_exact\"] = route_pd_df[\"DAYOFSERVICE\"] + pd.Series(map(seconds_to_time, route_pd_df[\"PLANNEDTIME_ARR\"]), name=\"time\")\n",
    "\n",
    "        # get the directions\n",
    "        route_pd_df = pd.merge(route_pd_df ,trips_df) \n",
    "\n",
    "        # merge the weather and bus data. then remove the \n",
    "        total_df = pd.merge(route_pd_df, weather_df, left_on = \"date_and_time\", right_on = \"dt\")\n",
    "        # drop any duplicates with respect to the time and tripid,\n",
    "        # should be unique\n",
    "        total_df.drop_duplicates([\"TRIPID\", \"datetime_exact\"], inplace = True)\n",
    "        # removing outliers\n",
    "        weather_train_no_out = total_df[total_df[\"delay_amount\"] < 1800]\n",
    "        # setting up the training data fro the models\n",
    "        weather_train_target = weather_train_no_out[\"delay_amount\"]\n",
    "        # for linear regression we cannot use the weekdays encoded as [1-7] \n",
    "        # as it implies a heirarchy\n",
    "        # so we must use the one that is one hot encoded\n",
    "        # see if we wnat humidity in the mix\n",
    "        training = weather_train_no_out[[\"PROGRNUMBER\", \"planned_arr_hours\", \"planned_arr_minutes\", \"holiday\",  \"DIRECTION\", \"humidity\", \"rain_1h\"]]\n",
    "\n",
    "        # if we one hot encode the days of the week\n",
    "        train_days = loaded_encoder.transform(weather_train_no_out[\"day_of_week\"].values.reshape(-1,1))\n",
    "        # encode the wetaher types\n",
    "        weather_train = weather_encoder.transform(np.array(list(weather_train_no_out[\"weather_main\"])).reshape(-1,1))\n",
    "\n",
    "        # design matrix, the training matrix\n",
    "        design_matrix = np.concatenate((training.values, train_days.toarray(), weather_train.toarray()), axis = 1)\n",
    "\n",
    "        # training the model\n",
    "        rf_weather = RandomForestRegressor(n_estimators = 100, max_depth = 5, n_jobs = -1, oob_score = True)\n",
    "        rf_weather.fit(design_matrix, weather_train_target)\n",
    "        filename = 'models/rf_{}.sav'.format(route)\n",
    "        pickle.dump(rf_weather, open(filename, 'wb'))\n",
    "\n",
    "        # take into consideration if we run into memory error, as resources are sharedc\n",
    "        # so that we know which files tro extract to the loacl device to train the models\n",
    "    except MemoryError:\n",
    "        # so that we can know which routes need to be trained locally\n",
    "        # as iut caused a memopry error on the server\n",
    "        print(route, \"had an error with memory. train locally\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
